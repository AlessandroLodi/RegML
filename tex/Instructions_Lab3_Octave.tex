\documentclass[DIN, pagenumber=false, fontsize=11pt, parskip=half]{scrartcl}

\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{framed}
\usepackage{hyperref}


% for matlab code
% bw = blackwhite - optimized for print, otherwise source is colored
\usepackage[framed,numbered,bw]{mcode}

\lstset{breakatwhitespace=false}

% for other code
%\usepackage{listings}

\setlength{\parindent}{0em}

% set section in CM
\setkomafont{section}{\normalfont\bfseries\Large}

\newcommand{\mytitle}[1]{{\noindent\LARGE\textbf{#1}}}
%\newcommand{\mysection}[1]{\large\textbf{\section*{#1}}}
\newcommand{\mysection}[1]{\noindent\large\textbf{#1}}
\newcommand{\mysubsection}[2]{\romannumeral #1) #2}

%%%% Operator norm
\DeclareFontEncoding{FMS}{}{}
\DeclareFontSubstitution{FMS}{futm}{m}{n}
\DeclareFontEncoding{FMX}{}{}
\DeclareFontSubstitution{FMX}{futm}{m}{n}
\DeclareSymbolFont{fouriersymbols}{FMS}{futm}{m}{n}
\DeclareSymbolFont{fourierlargesymbols}{FMX}{futm}{m}{n}
\DeclareMathDelimiter{\VERT}{\mathord}{fouriersymbols}{152}{fourierlargesymbols}{147}

%===================================
\begin{document}

\noindent\textbf{Regularization Methods for Machine Learning} \hfill Jun 21, 2018\\
RegML 2018, Genova, Italy \\ \rule{\textwidth}{1pt}

\mytitle{LAB 3: Sparsity-based learning}\\
\rule{\textwidth}{1pt}
\begin{itemize}\itemsep1pt \parskip0pt \parsep0pt
  \item This lab is about feature selection within the framework of sparsity based regularization, using elastic net regularization.
  \item The aim of the lab is to play with the libraries and to get a practical grasp of what we have discussed in class.
  \item Follow the instructions below. 
\end{itemize}

\begin{framed}
\textbf{\textbf{Goal}:} \\
This lab is divided in two parts depending of their level of complexity (\textbf{Beginner}, \textbf{Intermediate}). Your goal is to complete entirely, at least, one of the two parts.
\end{framed}

\begin{framed}
\textbf{\textbf{Setup instructions}:} \\
\textit{Running OCTAVE}

 Download the file \texttt{regml2018\_lab3.zip} from the syllabus on the course website (\url{http://lcsl.mit.edu/courses/regml/regml2018/\#syllabus}), extract it and add all the sub-folders to the OCTAVE path. This file includes all the code you need!
\end{framed}

%\pagebreak
\mysection{Toy problem}

We focus on a regression problem where the target function is linear. We will consider synthetic data generated (randomly sampled) according to a given probability distribution and affected by noise. You will have the possibility of controlling the sizes of training and test sets, data dimensionality and number of relevant features.

\begin{framed}
\textbf{NOTE:} \\
In the code we use a different notation from what you have seen in the classes. The minimized functional is:
\begin{equation*}
\min_{\beta\in \mathbb{R}^p} \  \frac{1}{2n} \|X\beta-Y\|^2 +
\texttt{L2\_par} \frac{\VERT X \VERT^2}{2n}\|\beta\|^2_2 +
\texttt{L1\_par}\|\beta\|_1 ,
\end{equation*}
where $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^n$.

In addition, in some  scripts the sparsity parameter \texttt{L1\_par} can be also referenced as \texttt{tau}, and the smoothing parameter \texttt{L2\_par} as \texttt{smooth\_par}.
\end{framed}


\newpage
\begin{center}
\large\textbf{PART I: Beginner}
\end{center}

\mysection{Overture: Warm up}

\noindent Open the files \texttt{loadSparseDataset\_octave.m} and \texttt{learnSparse\_octave.m} in Octave.
Look at the code, and see that the first lines are dedicated to the set up of various options and parameters.
The objective of this section is to familiarize you with the code.

\begin{itemize}
	\item \textbf{I.A} In \texttt{loadSparseDataset\_octave.m}, look at the parameters and generate a training set with the default parameters.
	Run then \texttt{learnSparse\_octave.m}, with the default parameters, to start a training phase.
	The first figure displays the cross validation error together with the test error.
	The second figure displays the sparsity of the reconstructed signal depending on the chosen \texttt{L1\_par}.
	Compare the sparsity of the reconstructed signal with the original one.\\
	\textbf{Note:} In this lab we perform regression and not classification. So the error is not a percentage error on the labels,  but is measured with quantities of the form $(1/n)\Vert X \beta_{pred} - Y \Vert^2$. 
	
	\item \textbf{I.B} Train your dataset by changing now the values for \texttt{L1\_par} and \texttt{L2\_par} in \texttt{learnSparse\_octave.m}.
	\begin{itemize}
		\item \textbf{I.B1} With $\texttt{L2\_par}=0$ and playing with \texttt{L1\_par}, try to obtain a sparser or denser solution. How does the sparsity behave with respect to \texttt{L1\_par}?
		\item \textbf{I.B2} Repeat the experiment with a $\texttt{L2\_par}>0$. How do test error and number of selected features vary?
\end{itemize}		
	
	\item \textbf{I.C} Repeat the experiments of I.B1, by considering this time a noisy dataset (you can take a noise parameter equal to 0.3).
	How does now behave the sparsity? 
	What can you say about the training error?
	
\end{itemize}




\mysection{Allegro con brio: Analysis}

Carry on the following experiments either using \texttt{loadSparseDataset\_octave.m} and \texttt{learnSparse\_octave.m}, when it is possible, or writing appropriate scripts.

\begin{itemize}
	
	
	\item \textbf{I.D} 	Back on the shell, have a look to the content of directory \texttt{./PROXIMAL\_} \texttt{TOOLBOXES/L1L2\_TOOLBOX}. 
	There you will find, among others, the code for  \mcode{l1l2\_algorithm} (used for variable selection), \mcode{l1l2\_kcv} (used for model selection with kcv or loo), \mcode{l1l2\_pred} (for prediction on a test set).

For more information about the parameters and the usage of those scripts, use the help.
	
	
Finally, you may want to have a look at file \texttt{l1l2\_demo\_simple.m} for a complete example of analysis.
You might use it as a basis for writing your own script(s).

	\item \textbf{I.E} \emph{(Prediction and selection)}
	Considering elastic net regularization, we want to observe, in a systematic way, how the training and test errors are sensitive to the parameters, as well as the number of relevant features of the solution (changing only one parameter at a time).
	To achieve this, you should try to pick some parameters, or run iteratively over a range of parameters, by exploiting the code contained in \texttt{l1l2\_demo\_simple.m}.

\begin{itemize}
  \item \textbf{I.E1} When we change the  regularization parameter \mcode{L1_par} associated with the $\ell_1$-norm.
  \item \textbf{I.E2} When we change (increase or decrease) the regularization parameter \mcode{L2_par} associated with the $\ell_2$-norm.\\
  \textbf{Hint:} Try the following fixed parameters: 20 points of dimension 100, with 15 relevant features, a noise level equal to 1 and \mcode{L1_par}=0.1.
  \item \textbf{IE.3} The training set size grows (this is not the same than generating different training sets with an increasing size!).\\
  \textbf{Hint:} Try the same parameters as above, with \mcode{L2_par}=0. 
  \item \textbf{I.E4} The amount of noise on the generated data grows (the test set is generated with the same parameter of the training).
\end{itemize}


	

	\item \textbf{I.F} \emph{(Large $p$ and small $n$)} Perform experiments similar to those above changing $p$ (dimensionality of the points), $n$ (number of training points) and $s$ (number of relevant variables).
	In particular, when $p\gg n$, look at how the result behave, depending whether $s<n$ holds or no. (try e.g $n=80$ and $p=300$). Try to identify different regimes.

%\begin{itemize}
% \item Set $p\ll n$ and $s>n$ (is it possible?)
%  \item Set $p\gg n$ and $s>n$
%  \item Set $p\gg n$ and $s<n$
%\end{itemize}

\end{itemize}





\begin{center}
\large\textbf{PART II: Intermediate}
\end{center}

\mysection{Crescendo: Data standardization}

From now you can only use your own scripts.

Import the classification dataset given in \texttt{part3-data.mat}, which contains two matrices: \mcode{X} and \mcode{Y}.
For this you can just double click on it in the file explorer, otherwise do:\\
\mcode{temp=load('part3-data.mat'); X=temp.X; Y=temp.Y;}

Here $X \in \mathbb{R}^{n \times p}$ is a matrix containing $n$ points in $\mathbb{R}^p$, and $Y \in \{-1;+1\}^n$ is generated from $Y=X \beta$, where $\beta \in \mathbb{R}^p$ is a sparse vector, meaning that it has $s$ nonzero components, with $s$ being small.

\begin{itemize}
  \item \textbf{II.A} You do not know how many relevant features generated this data.
  Try to get an estimation of this number $s$, by using \mcode{l1l2_learn}, according to your observations in part I.
  
  \item \textbf{II.B} An other way to try to estimate $s$ is to measure the correlation between the columns of $X$ and $Y$.
  Indeed, the zero coefficients in $\beta$ will ignore the corresponding columns in $X$ while generating $Y$. 
  To do so, you might use\\
  \mcode{c=abs(corr(X,Y)); stairs(c);}\\
  If you do not see which columns of $X$ are the more correlated with $Y$, you can use \mcode{[\~,I]=sort(c);} to sort its values and identify the more relevant features.
You should have now a precise idea of what is $s$.
Can you also identify which are these $s$ features?  
  
  \item \textbf{II.C} Use again \mcode{l1l2_learn} and tune the sparsity parameter \texttt{L1\_par} in view to select only $s$ features ($s$ being your estimation from the previous question). 
  Look at which are the selected features in this solution.
  Do they correspond to the ones you identified in II.B?
  If no, can you figure out why?\\
  % \textbf{Hint:} Analyse the correlation between the columns of \mcode{X} and \mcode{Y} and the ranges of the columns of \mcode{X}, e.g. with \mcode{imagesc(X(:,1:10)); colorbar}.
\end{itemize}

%\begin{center}
%\large\textbf{PART III: Advanced}
%\end{center}
%
%\mysection{Finale: Challenges}
%
%\textbf{Challenge 1:} Classification performances on microarray expression data
%
%In the file \texttt{part4-data.mat} you can find a dataset of microarray expression data. The original dataset is available \href{http://www.broadinstitute.org/cancer/pub/all_aml/}{here}, the given data has been extracted from the example data of \href{https://pypi.python.org/pypi/L1L2Signature}{L1L2 Signature}. The dataset contains 20 examples each of which reports the expression levels of 7129 genes. The goal is to distinguish between examples of acute lymphoblastic leukemia and acute myeloid leukemia and to select the set of meaningful genes for this task. The goal of this part is to maximize the precision of the classification algorithm. Have a look at the script \texttt{demo1\_lab3.m}. This script contains a code snippet to perform feature selection using the previously presented MATLAB scripts.
%
%\textbf{Challenge 2:} Quality of the selected features on a function approximation problem
%
%Consider the dataset in \texttt{part5-data.mat}. The dataset has been extracted from a generalized linear model. The goal of this part is to submit a complete list of functions that are correlated with the regression task. Have a look at the script \texttt{demo2\_lab3.m}. This script contains a code snippet to perform feature selection using the previously presented MATLAB scripts.
%
%\begin{framed}
%\textbf{Submission:}
%You should drop your results in a MATLAB matrix file named \texttt{name-surname-1.mat} (for the first challenge) and \texttt{name-surname-2.mat} (for the second challenge) to the link: \url{http://www.dropitto.me/regml} with password \texttt{regml2014} by the end of the challenge session. The results are presented during the next class and the first five will be awarded by an \textbf{awesome gift}.
%
%\textbf{Deadline:} 6:00 PM.
%\end{framed}

\end{document} 