\documentclass[DIN, pagenumber=false, fontsize=11pt, parskip=half]{scrartcl}

\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{framed}
\usepackage{hyperref}


% for matlab code
% bw = blackwhite - optimized for print, otherwise source is colored
\usepackage[framed,numbered,bw]{mcode}

\lstset{breakatwhitespace=false}

% for other code
%\usepackage{listings}

\setlength{\parindent}{0em}

% set section in CM
\setkomafont{section}{\normalfont\bfseries\Large}

\newcommand{\mytitle}[1]{{\noindent\LARGE\textbf{#1}}}
%\newcommand{\mysection}[1]{\large\textbf{\section*{#1}}}
\newcommand{\mysection}[1]{\noindent\large\textbf{#1}}
\newcommand{\mysubsection}[2]{\romannumeral #1) #2}


%===================================
\begin{document}

\noindent\textbf{Regularization Methods for Machine Learning} \hfill Jun 19, 2018\\
RegML 2018, Genova, Italy\\ \rule{\textwidth}{1pt}

\mytitle{\center LAB 2: Spectral filters \& multi-class classification}\\
\rule{\textwidth}{1pt}
\begin{itemize}\itemsep1pt \parskip0pt \parsep0pt
  \item This laboratory is about binary and multi-class classification and model selection on synthetic as well as real data, focusing on the role and on the properties of spectral filters.
  \item The aim of the lab is to play with the libraries and get a practical grasp of what we have discussed in class.
  \item Follow the instructions below.  
\end{itemize}

\begin{framed}
\textbf{\textbf{Goal}:} \\
This lab is divided in three parts depending of their level of complexity (\textbf{Beginner}, \textbf{Intermediate}, \textbf{Advanced}). Your goal is to entirely complete at least one of the three parts.
\end{framed}

\begin{framed}
\textbf{\textbf{Setup instructions}:} \\
\textit{Running OCTAVE}
 Download the file \texttt{regml2018\_lab2.zip} from the syllabus on the course website (\url{http://lcsl.mit.edu/courses/regml/regml2018/\#syllabus}), extract it and add all the sub-folders to the OCTAVE path. This file includes all the code you need!
\end{framed}

\pagebreak
\begin{center}
\large\textbf{PART I: Beginner}
\end{center}

\mysection{Overture: Warm up}

\noindent Open the files \texttt{loadDataset\_octave.m} and \texttt{learn\_octave.m} in Octave.
Look at the code, and see that the first lines are dedicated to the set up of various options and parameters.
The objective of this section is to familiarize you with the code.

\begin{itemize}
	\item \textbf{I.A} In \texttt{loadDataset\_octave.m}, look at the parameters and modify them to generate a simulated classification dataset of type \textit{Spiral}.
	To do so, after changing the parameters and saving \texttt{loadDataset\_octave.m}, execute it directly from Octave's interface, or type \texttt{loadDataset\_octave;} in the shell.\\
	\textbf{Tip:} If too many figures are opened at the same time, you can close them all at once with the command \mcode{close all}.
	\item \textbf{I.B} 
	Use \texttt{learn\_octave.m} to find a  classifier for this dataset.
	For this, take a \textit{Truncated SVD} filter, associated to a \textit{Gaussian} kernel.
	Using a Gaussian Kernel requires to select a parameter \mcode{sigma}, let the code choosing it automatically.
	Have a look at the parameter selection part and the various options of KCV. To select the regularization parameter you can either choose KCV or set a fixed value.
	To run the learning algorithm, save \texttt{learn\_octave.m}, and execute it directly from Octave's interface, or type \texttt{learn\_octave;} in the shell.
	Observe then the plot of the KCV error and the balance between training and test errors.
	Also have a look at data set, where a separation function has now appeared.
\end{itemize}

\begin{framed}
\textbf{NOTE:} \\
Remember that in the MATLAB code the regularization parameter for the \emph{Tikhonov} (\texttt{rls.m}), \emph{T-SVD} (\texttt{tsvd.m}) and \emph{cut-off} (\texttt{cutoff.m}) is called \mcode{t} instead of $\lambda$. For the \emph{NU-Method} (\texttt{nu.m}) and the \emph{Landweber-Method} (\texttt{land.m}), the regularization parameter \mcode{t} is the number of iterations, that is, roughly speaking, $\mcode{t} \sim \frac{1}{\lambda}$.
\end{framed}




\mysection{Allegro con brio: Analysis}

Carry on the following experiments either using \texttt{loadDataset\_octave.m} and \texttt{learn\_octave.m}, when it is possible, or writing appropriate scripts.

\begin{itemize}
	\item \textbf{I.C} Back on the shell, check the content of directory \texttt{./spectral\_reg} \texttt{\_toolbox}. 
	There you will find, among others, the code for command \mcode{learn} (used for training), \mcode{patt\_rec} (used for testing), \mcode{kcv} (used for model selection on the training set).
For more information about the parameters and the usage of those scripts, type:

\mcode{help learn}\\
\mcode{help patt_rec}\\
\mcode{help kcv}


Finally, you may want to have a look at the content of directory \texttt{./dataset\_scripts} and in particular to file \texttt{create\_dataset.m}, that  allows you to generate synthetic data of different kinds.

\item \textbf{I.D} Generate noisy data of \emph{Spiral} type. Consider three algorithms, namely \emph{RLS}, \emph{Truncated SVD} and \emph{NU-Method}. Observe how the training and test errors change as:
	
\begin{itemize}
  \item We change (increase or decrease) the regularization parameter \mcode{t}.
  \item The training set size grows (try various choices of \mcode{n} as long as your computer supports you!).
  \item The amount of errors in the labels in the generated data grows.
\end{itemize}
Run training and testing for various choices of the suggested parameters.

\item \textbf{I.E} Leaving all the other parameters fixed, choose an appropriate range for the regularization parameter, \mcode{tval=[t\_min:t\_step:t\_max]}, and plot the training and the test errors for each \mcode{t}. 
For doing this, you might use the functions that you saw in I.C:

\mcode{[Xtr, Ytr] = create_dataset(..);}\\
%[Xtr, Ytr, Xtest, Ytest] = loadDataset(... );}\\
\mcode{[alpha, err] = learn(.., .., .., t, Xtr, Ytr, 'class');}\\
\mcode{training_error = cell2mat(err);}

\mcode{[Xtest, Ytest] = create_dataset(..);}\\
\mcode{[\~,test_error] = patt_rec(..,..,alpha, Xtr, Xtest, Ytest, 'class');}

	Use the KCV option to select by cross-validation the optimal regularization parameter, and see how it relates to your previous plot.
	
	\item \textbf{I.F} Leaving all the other parameters fixed, choose an appropriate range for the number of points in the training set, \mcode{nval=[n\_min:} \mcode{n\_step:n\_max]},  and plot the training and test errors.
	 What do you observe as \mcode{n} $\rightarrow \infty$? How do the different regularization parameters affect the learning process? Which are the main differences in terms of regularization between the methods?
\end{itemize}




\begin{center}
\large\textbf{PART II: Intermediate}
\end{center}

\mysection{Crescendo: Advanced Analysis}

Carry on the following experiments either using \texttt{loadDataset\_octave.m} and \texttt{learn\_octave.m}, or writing appropriate scripts.
 In this part you have to focus more on the effects of regularization and on the correct choice of \mcode{sigma}.
 
\begin{itemize}
	\item \textbf{II.A} Consider \emph{gaussian-RLS} and perform parameter tuning in this case. This time, together with the regularization parameter \mcode{t}, you'll have to choose an appropriate \mcode{sigma}, the kernel parameter.
\begin{itemize}
  \item Try some (\mcode{sigma}, \mcode{t}) pairs and compare the obtained training error  and test error.
  \item Fix \mcode{t} and observe the effect of changing \mcode{sigma}.
  \item Fix \mcode{sigma} and observe the effect of changing \mcode{t}.
  \item Do you notice (and if so, when) any overfitting/oversmoothing effects?
\end{itemize}
  If you have time, you can try to plot the test error for a range of couples (\mcode{sigma}, \mcode{t}).

	\item \textbf{II.B} Compare \emph{RLS} with \emph{NU-Method} on a kernel of your choice.
\begin{itemize}
  \item Tune the parameters by using KCV.
  \item Compare the time needed to obtain a solution.
  \item Compare the training and test errors.
\end{itemize}
\end{itemize} 
 
 
 


\begin{center}
\large\textbf{PART III: Advanced}
\end{center}

\mysection{Finale: The Challenge}

The challenge consists in a learning task using a real dataset, namely \emph{USPS}. This dataset contains a number of handwritten digits images. The problem is to train \emph{the best classifiers} that are able to discriminate between digits \fbox{\texttt{3}}, \fbox{\texttt{8}} and \fbox{\texttt{0}}.

Have a look at the script \texttt{demo\_lab2.m}. This script contains a code snippet to perform a multi-class classification task using the previously presented scripts (see I.C).

You should understand what the scripts are supposed to do, and train the classifiers in order to perform One vs. All classification for all the combinations of the digits \texttt{3}, \texttt{8} and \texttt{0}.

Once the classifiers have been trained, the model must be exported in a matrix file by means of the \texttt{save\_challenge\_lab2.m} script (to see how to use it please try the command \mcode{help save_challenge_lab2}.

\begin{framed}
\textbf{Submission:}
You should drop your results in a matrix file named \texttt{name-surname} to the link: \url{https://www.dropbox.com/request/DWcrqDPQEulg7Dh0vqlz} by the end of the challenge session.\\
The results will be presented during the next class. The score is based on the accuracy of the classifier on a \textit{completely independently sampled} test set.

\textbf{Deadline:} 6:00 PM.
\end{framed}

\end{document} 