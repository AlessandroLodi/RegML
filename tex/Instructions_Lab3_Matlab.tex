\documentclass[DIN, pagenumber=false, fontsize=11pt, parskip=half]{scrartcl}

\usepackage[english]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{framed}
\usepackage{hyperref}
\usepackage{xspace}
\newcommand{\MATLAB}{\textsc{Matlab}\xspace}


% for matlab code
% bw = blackwhite - optimized for print, otherwise source is colored
\usepackage[framed,numbered,bw]{mcode}

\lstset{breakatwhitespace=false}

% for other code
%\usepackage{listings}

\setlength{\parindent}{0em}

% set section in CM
\setkomafont{section}{\normalfont\bfseries\Large}

\newcommand{\mytitle}[1]{{\noindent\LARGE\textbf{#1}}}
%\newcommand{\mysection}[1]{\large\textbf{\section*{#1}}}
\newcommand{\mysection}[1]{\noindent\large\textbf{#1}}
\newcommand{\mysubsection}[2]{\romannumeral #1) #2}
%%%% Operator norm
\DeclareFontEncoding{FMS}{}{}
\DeclareFontSubstitution{FMS}{futm}{m}{n}
\DeclareFontEncoding{FMX}{}{}
\DeclareFontSubstitution{FMX}{futm}{m}{n}
\DeclareSymbolFont{fouriersymbols}{FMS}{futm}{m}{n}
\DeclareSymbolFont{fourierlargesymbols}{FMX}{futm}{m}{n}
\DeclareMathDelimiter{\VERT}{\mathord}{fouriersymbols}{152}{fourierlargesymbols}{147}


%===================================
\begin{document}

\noindent\textbf{Regularization Methods for Machine Learning} \hfill Jul 2, 2020\\
RegML 2020, Genova, Italy \\ \rule{\textwidth}{1pt}

\mytitle{LAB 3: Sparsity-based learning}\\
\rule{\textwidth}{1pt}
\begin{itemize}\itemsep1pt \parskip0pt \parsep0pt
  \item This lab is about feature selection within the framework of sparsity based regularization, using elastic net regularization.
  \item The aim of the lab is to play with the libraries and to get a practical grasp of what we have discussed in class.
  \item Follow the instructions below.
\end{itemize}

\begin{framed}
\textbf{\textbf{Goal}:} \\
This lab is divided in two parts depending on their level of complexity (\textbf{Beginner}, \textbf{Intermediate}).
Your goal is to entirely complete at least one of the two parts.
\end{framed}

\begin{framed}
\textbf{\textbf{Setup instructions}:} \\
\textit{Running \MATLAB}

 Download the file \texttt{regml2018\_lab3.zip} from the syllabus on the course website (\url{http://lcsl.mit.edu/courses/regml/regml2018/\#syllabus}), extract it and add all the sub-folders to the \MATLAB path. This file includes all the code you need!
\end{framed}

%\pagebreak
\mysection{Toy problem}

We focus on a regression problem where the target function is linear.
We will consider synthetic data that is generated (randomly sampled) according to a given probability distribution and affected by noise.
You will be able to control the sizes of training and test sets, dimensionality of the data and the number of relevant features.

\begin{framed}
\textbf{NOTE:} \\
In the code we use different notation from what you have seen in class. Namely, we consider the minimization problem
\begin{equation*}
\min_{\beta\in \mathbb{R}^p} \  \frac{1}{2n} \|X\beta-Y\|^2 +
\texttt{L2\_par} \frac{\VERT X \VERT^2}{2n}\|\beta\|^2_2 +
\texttt{L1\_par}\|\beta\|_1 ,
\end{equation*}
where $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^n$.

Furthermore, the sparsity parameter \texttt{L1\_par} is, in some of the code, denoted by \texttt{tau}, while the smoothing parameter \texttt{L2\_par} can be reffered to as \texttt{smooth\_par}.
\end{framed}














\newpage
\begin{center}
\large\textbf{PART I: Beginner}
\end{center}

\mysection{Overture: Warm up}

Run the file \texttt{gui\_l1l2.m} and the GUI will start. Have a look at the various components.

\begin{itemize}
	\item \textbf{I.A} Generate a training set with the default parameters.
	Press the \fbox{\textsf{run}} button to start a training phase with the selected \texttt{L1\_par} and \texttt{L2\_par} parameters and perform testing.
	The first figure displays the cross validation error together with the test error.
	The second figure displays the sparsity of the reconstructed signal for the given \texttt{L1\_par}.
	Compare the sparsity of the reconstructed signal with that of the original signal.\\
	\textbf{Note:} In this lab we perform regression, not classification.
  Therefore, the error is no longer measured as a percentage of wrongly assigned labels, but is rather measured by quantities of the form $(1/n)\Vert X \beta_{pred} - Y \Vert^2$.

	\item \textbf{I.B} Train your dataset by changing the values of \texttt{L1\_par} and \texttt{L2\_par} in \texttt{learnSparse\_octave.m}.
	\begin{itemize}
		\item \textbf{I.B1} Fixing $\texttt{L2\_par}=0$ and playing with \texttt{L1\_par}, try to obtain a sparser or denser solution.
    How does the sparsity behave with respect to \texttt{L1\_par}?
		\item \textbf{I.B2} Repeat the experiment with $\texttt{L2\_par}>0$.
    How do the test error and the number of selected features vary?
\end{itemize}

	\item \textbf{I.C} Repeat the experiments of I.B1, but this time considering a noisy dataset (you can set the noise parameter to 0.3).
	How does this affect the sparsity?
	What can you say about the training error?

\end{itemize}



\mysection{Allegro con brio: Analysis}

In the following experiments either use \texttt{loadSparseDataset\_octave.m} and \texttt{learnSparse\_octave.m}, or, when needed, write the required code.

\begin{itemize}


	\item \textbf{I.D} 	Have a look at the contents of the directory \texttt{./PROXIMAL\_} \texttt{TOOLBOXES/L1L2\_TOOLBOX}.
	There you will find, among other things, the code for  \texttt{l1l2\_algorithm} (used for variable selection), \texttt{l1l2\_kcv} (used for model selection with kcv or loo), and \texttt{l1l2\_pred} (used for prediction on a test set).

For more information regarding the parameters and the usage of those functions, use the help.

Finally, you may want to have a look at the file \texttt{l1l2\_demo\_simple.m} (in the directory \texttt{demo\_scripts}) for an example of the analysis.
You can use it as a basis for writing your own code.

	\item \textbf{I.E} \emph{(Prediction and selection)}
	Considering the elastic net regularization, we want to study the sensitivity of training and test errors with respect to the choice of regularization parameters and with respect to the number of relevant features of the solution, by changing one parameter at a time.
	To that end, you should try to pick some parameters, or run them in a loop, by exploiting the code in \texttt{l1l2\_demo\_simple.m}.
  Namely, study what happens as

\begin{itemize}
  \item \textbf{I.E1} ... you change the  regularization parameter \texttt{L1\_par} associated with the $\ell_1$-norm.
  \item \textbf{I.E2} ... you change (increase or decrease) the regularization parameter \texttt{L2\_par} associated with the $\ell_2$-norm.\\
  \textbf{Hint:} Try the following fixed parameters: 20 points of dimension 100, with 15 relevant features, a noise level equal to 1 and \texttt{L1\_par}=0.1.
  \item \textbf{IE.3} ... the size of the training set grows (this is not the same as generating different training sets of increasing size!).\\
  \textbf{Hint:} Try the same parameters as above, with \texttt{L2\_par}=0.
  \item \textbf{I.E4} ... the amount of noise on the generated data grows (the test set is generated with the same parameters as the training set).
\end{itemize}




	\item \textbf{I.F} \emph{(Large $p$, small $n$)} Perform experiments similar to those above but now changing $p$ (dimensionality of the points), $n$ (number of training points) and $s$ (number of relevant variables).
	In particular, look at how do the results behave when $p\gg n$, depending whether $s<n$ holds or not (e.g. try $n=80$ and $p=300$).
  Try to identify different regimes.

%\begin{itemize}
% \item Set $p\ll n$ and $s>n$ (is it possible?)
%  \item Set $p\gg n$ and $s>n$
%  \item Set $p\gg n$ and $s<n$
%\end{itemize}

\end{itemize}














\begin{center}
\large\textbf{PART II: Intermediate}
\end{center}

\mysection{Crescendo: Data standardization}

From now on do not use the GUI.

Import the classification dataset given in \texttt{part3-data.mat}, which contains two matrices: \texttt{X} and \texttt{Y}.
For this you can just double click on it in the file explorer, or otherwise write\\
\mcode{temp=load('part3-data.mat'); X=temp.X; Y=temp.Y;}

Here $X \in \mathbb{R}^{n \times p}$ is a matrix containing $n$ points in $\mathbb{R}^p$, and $Y \in \{-1;+1\}^n$ is generated from $Y=X \beta$, where $\beta \in \mathbb{R}^p$ is an $s$-sparse vector, meaning that it has $s$ nonzero components, with $s$ being small.

\begin{itemize}
  \item \textbf{II.A} You do not know how many relevant features generated this data.
  Try an estimate this number $s$, by using \texttt{l1l2\_learn}, according to your observations in part I.

  \item \textbf{II.B} An other way to try to estimate $s$ is to measure the correlation between the columns of $X$ and $Y$.
  Indeed, the zero coefficients in $\beta$ will ignore the corresponding columns in $X$ while generating $Y$.
  To do so, you might use\\
  \mcode{c=abs(corr(X,Y)); stairs(c);}\\
  If you do not see which columns of $X$ are more correlated with $Y$, you can use \mcode{[\~,I]=sort(c);} to sort and identify the more relevant features.
You should at this point have a precise idea of what is $s$.
Can you also identify which are these $s$ features?

  \item \textbf{II.C} Use again \texttt{l1l2\_learn} and tune the sparsity parameter \texttt{L1\_par} so that it selects only $s$ features ($s$ being your sparsity estimate from the previous question).
  Look at which are the selected features in your solution.
  Do they correspond to the ones you identified in II.B?
  If they do not, can you figure out why does that happen?\\
  % \textbf{Hint:} Analyse the correlation between the columns of \mcode{X} and \mcode{Y} and the ranges of the columns of \mcode{X}, e.g. with \mcode{imagesc(X(:,1:10)); colorbar}.
\end{itemize}

%\begin{center}
%\large\textbf{PART III: Advanced}
%\end{center}
%
%\mysection{Finale: Challenges}
%
%\textbf{Challenge 1:} Classification performances on microarray expression data
%
%In the file \texttt{part4-data.mat} you can find a dataset of microarray expression data. The original dataset is available \href{http://www.broadinstitute.org/cancer/pub/all_aml/}{here}, the given data has been extracted from the example data of \href{https://pypi.python.org/pypi/L1L2Signature}{L1L2 Signature}. The dataset contains 20 examples each of which reports the expression levels of 7129 genes. The goal is to distinguish between examples of acute lymphoblastic leukemia and acute myeloid leukemia and to select the set of meaningful genes for this task. The goal of this part is to maximize the precision of the classification algorithm. Have a look at the script \texttt{demo1\_lab3.m}. This script contains a code snippet to perform feature selection using the previously presented MATLAB scripts.
%
%\textbf{Challenge 2:} Quality of the selected features on a function approximation problem
%
%Consider the dataset in \texttt{part5-data.mat}. The dataset has been extracted from a generalized linear model. The goal of this part is to submit a complete list of functions that are correlated with the regression task. Have a look at the script \texttt{demo2\_lab3.m}. This script contains a code snippet to perform feature selection using the previously presented MATLAB scripts.
%
%\begin{framed}
%\textbf{Submission:}
%You should drop your results in a MATLAB matrix file named \texttt{name-surname-1.mat} (for the first challenge) and \texttt{name-surname-2.mat} (for the second challenge) to the link: \url{http://www.dropitto.me/regml} with password \texttt{regml2014} by the end of the challenge session. The results are presented during the next class and the first five will be awarded by an \textbf{awesome gift}.
%
%\textbf{Deadline:} 6:00 PM.
%\end{framed}

\end{document}
