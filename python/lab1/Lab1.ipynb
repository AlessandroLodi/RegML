{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1: Kernel Ridge Regression\n",
    "Authors: \n",
    "\n",
    "    Mathurin Massias (mathurin.massias@gmail.com)\n",
    "    \n",
    "    Giacomo Meanti  (giacomo.meanti@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "from lab1_utils import create_random_data, data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warmup\n",
    "\n",
    "### Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_random_data(n_samples=1000, noise_level=1, seed=932)\n",
    "print(\"%d samples, %d features\" % X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[y == -1][:,0], X[y == -1][:,1], alpha=0.5)\n",
    "ax.scatter(X[y == 1][:,0], X[y == 1][:,1], alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = data_split(X, y, n_train=800)\n",
    "print(\"%d training samples, %d test samples\" % (X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear ridge-regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_separation(X, Y, model):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = .02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z[Z < 0] = -1\n",
    "    Z[Z >= 0] = 1\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    \n",
    "    plt.scatter(X[Y == -1][:, 0], X[Y == -1][:, 1])\n",
    "    plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1])\n",
    "\n",
    "    \n",
    "def binary_classif_error(y_true, y_pred):\n",
    "    return np.mean(np.sign(y_pred) != y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 0.001\n",
    "\n",
    "model = KernelRidge(regularization, kernel=\"linear\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "test_preds = model.predict(X_test)\n",
    "\n",
    "print(\"Training error: %.2f%%\" % (binary_classif_error(y_train, train_preds) * 100))\n",
    "print(\"Test error: %.2f%%\" % (binary_classif_error(y_test, test_preds) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_separation(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the effect of different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Change the regularization parameter\n",
    "reg_values = np.geomspace(1e-4, 5e3, num=50)\n",
    "test_errors = []\n",
    "for reg in reg_values:\n",
    "    model = KernelRidge(reg, kernel=\"linear\")\n",
    "    model.fit(X_train, y_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_errors.append(binary_classif_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(reg_values, test_errors)\n",
    "ax.set_xlabel(\"Regularization\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Change in number of data-points\n",
    "# num_points = [1000, 2000, 3000, 4000, 5000]\n",
    "num_points = np.arange(500, 3000, 100)\n",
    "np_test_errors = []\n",
    "model = KernelRidge(1, kernel=\"linear\")\n",
    "for points in num_points:\n",
    "    X, y = create_random_data(points, 1, seed=932)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y, n_train=points - 200)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    np_test_errors.append(binary_classif_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_points, np_test_errors)\n",
    "ax.set_xlabel(\"Number of points\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Amount of noise in the data\n",
    "data_noise = [0.3, 0.5, 1.0, 2.0]\n",
    "noise_test_errors = []\n",
    "model = KernelRidge(1, kernel=\"linear\")\n",
    "for noise in data_noise:\n",
    "    X, y = create_random_data(1000, noise, seed=932)\n",
    "    X_train, X_test, y_train, y_test = data_split(X, y, n_train=800)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_preds = model.predict(X_test)\n",
    "    noise_test_errors.append(binary_classif_error(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(data_noise, noise_test_errors)\n",
    "ax.set_xlabel(\"Data noise\")\n",
    "ax.set_ylabel(\"Test error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Kernel ridge regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
