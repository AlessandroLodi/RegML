{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB3: Sparsity\n",
    "Author: Mathurin Massias (mathurin.massias@gmail.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lab3_utils import create_random_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation and model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "n_features = 200\n",
    "n_informative_features = 50\n",
    "\n",
    "X, y = create_random_data(n_samples, n_features, n_informative_features, \n",
    "                          noise_level=0.3)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "\n",
    "train_size = 0.8  # proportion of dataset used for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, shuffle=False, train_size=train_size)\n",
    "print(\"Training dataset shape:\", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, the objective function of the ElasticNet optimization is:\n",
    "$$\\frac{1}{2 \\times \\text{n_samples}} \\Vert y - X w \\Vert_2^2 + \\alpha \\times \\left( \\text{l1_ratio} \\times \\Vert w \\Vert_1 + \\frac{1 - \\text{l1_ratio}}{2} \\Vert w \\Vert_2^2\\right)$$\n",
    "\n",
    "See the docstring for more information in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ElasticNet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a classifier with arbitrary values for L1 and L2 penalization\n",
    "clf = ElasticNet(alpha=0.1, l1_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model and print some its first coefficients\n",
    "# beware that sklearn fits an intercept by default\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"50 first coefficients of estimated w:\\n\", clf.coef_[:50])\n",
    "print(\"Intercept: %f\" % clf.intercept_)\n",
    "print(\"Nonzero coefficients: %d\" % (clf.coef_ != 0.).sum())\n",
    "print(\"Testing error: %.4f\" % np.mean((y_test - clf.predict(X_test)) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the influence of l1_ratio on the sparsity of the solution\n",
    "l1_ratios = [0., 0., 0.]  # TODO choose your values\n",
    "\n",
    "train_errs = np.zeros(len(l1_ratios))\n",
    "test_errors = np.zeros_like(test_errs)\n",
    "\n",
    "for i, l1_ratio in enumerate(l1_ratios):\n",
    "    clf = # TODO\n",
    "    # TODO fit and check sparsity\n",
    "    # TODO compute train and test errors\n",
    "    train_errs[i] = \n",
    "    test_errs[i] = \n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(l1_ratios, test_errs, label='Test error')\n",
    "plt.plot(l1_ratios, train_errs, label='Train error')\n",
    "plt.xlabel(\"l1_ratio\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO also check the influence of alpha. \n",
    "# What happens when alpha becomes too big?\n",
    "alphas = np.geomspace(1e-4, 1e4, num=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection with cross validation\n",
    "In the next section, we use scikit-learn's built in functions to perform cross validated selection of alpha and l1_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load some data verifying $y = \\text{sign}(X w)$ where $w$ is $s$-sparse but you do not know $s$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"../../data/part3-data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"X\"]\n",
    "y = data[\"Y\"][:, 0]\n",
    "print(X.shape, y.shape)\n",
    "# TODO check numerically that y only contains 1s and -1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you must infer $s$.\n",
    "A first approach should be based on the Cross-Validation procedure used in the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO find optimal from a CV point of view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to try to estimate $s$ is to measure the correlation between\n",
    "the columns of $X$ and $y$. Indeed, the zero coefficients in $w$ will ignore the\n",
    "corresponding columns in $X$ while generating $y$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute correlation\n",
    "corr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort:\n",
    "idx = np.argsort()\n",
    "plt.plot(corr[idx[::-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO identify the cutoff numerically, get indices of highest correlated features\n",
    "highly_corr_feats = # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use again the code of the first part, to tune the sparsity parameter l1_ratio so that\n",
    "it selects only $s$ features ($s$ being your sparsity estimate from the previous\n",
    "question). Look at which are the selected features in your solution. Do they\n",
    "correspond to the ones you identified with the correlation approach? \n",
    "If they do not, can you figure out why does this happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
